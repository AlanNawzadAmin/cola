{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# CoLA Library Exercise\n",
    "\n",
    "This exercise is designed to help you get familiar with the CoLA linear algebra library. \n",
    "\n",
    "## Installation\n",
    "\n",
    "Make sure you have a Python 3.10+ environment with either JAX or PyTorch installed. You can install CoLA using pip:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/wilson-labs/cola.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alternatively, you can open the documentation in Colab and start working from there: [Quick Start](https://colab.research.google.com/github/wilson-labs/cola/blob/master/docs/notebooks/colabs/Quick_Start.ipynb)\n",
    "\n",
    "We strongly recommend that you read through the [documentation](https://cola.readthedocs.io/en/latest/index.html) to understand the library better.\n",
    "\n",
    "## Basic Exercises\n",
    "We'll start with some basic exercises to get you warmed-up for later. For each of the following you can use either JAX or PyTorch. We recommend that you try both and see if you spot any difference on the behavior.\n",
    "\n",
    "1. Create a `LinearOperator` using the `ops.Diagonal` and `ops.Dense` classes. Perform basic operations like addition, subtraction, and multiplication on these operators. Also print the dense version of the `LinearOperator`. Verify that the computations are correct by using the dense API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([3, 3])\n",
      "tensor([[ 1.0000,  0.5000,  0.0000],\n",
      "        [-0.5000,  1.0000, -0.2500],\n",
      "        [ 0.0000,  0.2500,  1.0000]])\n",
      "tensor([ 6., 15., 24.])\n",
      "tensor([ 6., 15., 24.])\n",
      "Dense+diag(tensor([-1.,  2.,  3.]))+-1*Tridiagonal\n",
      "tensor([[-1.0000,  1.5000,  3.0000],\n",
      "        [ 4.5000,  6.0000,  6.2500],\n",
      "        [ 7.0000,  7.7500, 11.0000]])\n",
      "tensor([[-1.0000,  1.5000,  3.0000],\n",
      "        [ 4.5000,  6.0000,  6.2500],\n",
      "        [ 7.0000,  7.7500, 11.0000]])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cola\n",
    "import torch\n",
    "\n",
    "dtype = torch.float32\n",
    "A = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], dtype=dtype)\n",
    "ones = torch.ones(size=(A.shape[0],), dtype=dtype)\n",
    "A_op = cola.ops.Dense(A)\n",
    "D_op = cola.ops.Diagonal(torch.tensor([-1., 2., 3.], dtype=dtype))\n",
    "alpha = torch.tensor([[-0.5, 0.25]], dtype=dtype).T\n",
    "beta = torch.tensor([[1., 1., 1.]], dtype=dtype).T\n",
    "T_op = cola.ops.Tridiagonal(alpha, beta, -alpha)\n",
    "\n",
    "\n",
    "print(A_op.dtype)\n",
    "print(A_op.shape)\n",
    "\n",
    "D = D_op.to_dense()\n",
    "T = T_op.to_dense()\n",
    "print(T_op.to_dense())\n",
    "\n",
    "print(A_op @ ones)\n",
    "print(A @ ones)\n",
    "\n",
    "print(A_op + D_op - T_op)\n",
    "print((A_op + D_op - T_op).to_dense())\n",
    "print(A + D - T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate a random normal matrix $A$ of size $N=100$ and a random unitary vector $b$ and then solve the following linear system $Ax=b$ using `cola.inverse`. Compare the answers with `jnp.solve` or `torch.solve`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.44e-15\n"
     ]
    }
   ],
   "source": [
    "from jax import numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "from jax.random import normal\n",
    "from jax.random import split\n",
    "\n",
    "N = 100\n",
    "key = PRNGKey(seed=21)\n",
    "A = normal(key, shape=(N, N))\n",
    "A_op = cola.ops.Dense(A)\n",
    "key = split(key, num=1)\n",
    "rhs = normal(key, shape=(N,))\n",
    "rhs /= jnp.linalg.norm(rhs)\n",
    "\n",
    "soln = cola.inverse(A_op) @ rhs\n",
    "\n",
    "soln_jax = jnp.linalg.solve(A, rhs)\n",
    "abs_diff = jnp.linalg.norm(soln - soln_jax)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get a random symmetric matrix $S$ of size $N=1,000$ by multiplying $S=A A^T$ where $A$ is a random normal matrix and a random unitary vector $b$ and solve $Sx=b$. Use `ops.PSD` to decorate $S$. Through `cola.inverse` find the solution. What algorithm is being use in this case? How can you modify some of the hyperparameters of these algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.62e-13\n"
     ]
    }
   ],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "N = 100\n",
    "key = PRNGKey(seed=21)\n",
    "dtype = jnp.float64\n",
    "A = normal(key, shape=(N, N))\n",
    "mu = 1.e-1  # ensure PSD\n",
    "S = A @ A.T + mu * jnp.eye(A.shape[0])\n",
    "# S_op = cola.ops.Dense(S)  # compare to option below\n",
    "S_op = cola.ops.PSD(cola.ops.Dense(S))\n",
    "key = split(key, num=1)\n",
    "rhs = normal(key, shape=(N,))\n",
    "rhs /= jnp.linalg.norm(rhs)\n",
    "\n",
    "soln = cola.inverse(S_op, method=\"cg\", tol=1e-10, max_iters=10_000) @ rhs\n",
    "\n",
    "soln_jax = jnp.linalg.solve(S, rhs)\n",
    "abs_diff = jnp.linalg.norm(soln - soln_jax)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find the eigenvalues and eigenvectors of the $T$ matrix constructed here: \n",
    "[Linear Operators: What and Why?](https://github.com/wilson-labs/cola/blob/main/docs/notebooks/LinOpIntro.ipynb). Compare the solutions obtained from decorating $T$ with `ops.Symmetric` and not decorating it. Are different algorithms being used? Is there a runtime benefit from dispatching a different algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00e+00\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "dtype = torch.float64\n",
    "alpha = torch.ones(size=(N - 1, 1), dtype=dtype)\n",
    "beta = -2 * torch.ones(size=(N, 1), dtype=dtype)\n",
    "T_op = cola.ops.Tridiagonal(alpha, beta, alpha)\n",
    "eigvals, eigvecs = cola.eig(T_op)\n",
    "eigvals_torch, eigvecs_torch = torch.linalg.eig(T_op.to_dense())\n",
    "abs_diff = torch.linalg.norm(eigvals - eigvals_torch)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Large Scale Machine Learning with CoLA\n",
    "\n",
    "Using JAX or PyTorch, pick any 3 out of the 5:\n",
    "\n",
    "### 1. GP\n",
    "\n",
    "GP Implement Gaussian Process (GP) inference with Radial Basis Function (RBF) kernel using `inverse()` from scratch on a dataset with at least 10k observations. You are not allowed to use GPyTorch. The formula for the GP posterior is:\n",
    "\n",
    "$$f_* | X, y, X_* \\sim \\mathcal{N}(\\mu_*, \\Sigma_*)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\mu_* = K(X_*, X)[K(X, X) + \\sigma^2_n I]^{-1}y$$\n",
    "\n",
    "$$\\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \\sigma^2_n I]^{-1}K(X, X_*)$$\n",
    "\n",
    "Here, $K$ is the RBF kernel, $X$ are the training inputs, $y$ are the training targets, $X_*$ are the test inputs, and $\\sigma^2_n$ is the noise variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-07 21:52:24--  https://www.andpotap.com/static/bike.mat\n",
      "Resolving www.andpotap.com (www.andpotap.com)... 69.164.216.245\n",
      "Connecting to www.andpotap.com (www.andpotap.com)|69.164.216.245|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 434829 (425K) [application/octet-stream]\n",
      "Saving to: ‘bike.mat’\n",
      "\n",
      "bike.mat            100%[===================>] 424.64K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-07-07 21:52:24 (11.2 MB/s) - ‘bike.mat’ saved [434829/434829]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O bike.mat \"https://www.andpotap.com/static/bike.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "def load_uci_data(data_dir, dataset, train_p=0.75, test_p=0.15):\n",
    "    file_path = os.path.join(data_dir, dataset + '.mat')\n",
    "    data = np.array(loadmat(file_path)['data'])\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2.0 * (X / X.max(0)[0]) - 1.0\n",
    "    y -= y.mean()\n",
    "    y /= y.std()\n",
    "\n",
    "    train_n = int(floor(train_p * X.shape[0]))\n",
    "    valid_n = int(floor((1. - train_p - test_p) * X.shape[0]))\n",
    "\n",
    "    split = split_dataset(X, y, train_n, valid_n)\n",
    "    train_x, train_y, valid_x, valid_y, test_x, test_y = split\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, valid_x, valid_y\n",
    "\n",
    "\n",
    "def split_dataset(x, y, train_n, valid_n):\n",
    "    train_x = x[:train_n, :]\n",
    "    train_y = y[:train_n]\n",
    "\n",
    "    valid_x = x[train_n:train_n + valid_n, :]\n",
    "    valid_y = y[train_n:train_n + valid_n]\n",
    "\n",
    "    test_x = x[train_n + valid_n:, :]\n",
    "    test_y = y[train_n + valid_n:]\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
    "\n",
    "\n",
    "train_x, train_y, *_ = load_uci_data(data_dir=\"./\", dataset=\"bike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Hessian Spectrum\n",
    "Compute the eigenspectrum of the Hessian of a pretrained neural network. You can download weights of image classifiers pretrained on CIFAR10. Use `cola.eig` or `cola.algorithms.lanczsos` and the spectral KDE smoothing method from [this paper](https://arxiv.org/pdf/1901.10159.pdf) to get a smoothed spectrum estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.nn.utils import _stateless\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, shuffle=False, num_workers=2)\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)\n",
    "model = model.to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def flatten_params(params):\n",
    "    shapes = [p.shape for p in params]\n",
    "    flat_params = torch.cat([p.flatten() for p in params])\n",
    "    return flat_params, shapes\n",
    "\n",
    "def unflatten_params(flat_params, shapes):\n",
    "    params = []\n",
    "    i = 0\n",
    "    for shape in shapes:\n",
    "        size = torch.prod(torch.tensor(shape)).item()\n",
    "        param = flat_params[i:i + size]\n",
    "        param = param.view(shape)\n",
    "        params.append(param)\n",
    "        i += size\n",
    "    return params\n",
    "\n",
    "flat_p, shape = flatten_params(list(model.parameters()))\n",
    "flat_p = flat_p.detach().requires_grad_(True)\n",
    "\n",
    "def stateless_model(fparams,x):\n",
    "    params = unflatten_params(fparams,shape)\n",
    "    names = list(n for n, _ in model.named_parameters())\n",
    "    nps=  {n: p for n, p in zip(names, params)}\n",
    "    return _stateless.functional_call(model,nps, x)\n",
    "\n",
    "def loss_fn(params):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    total_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        outputs = stateless_model(params, images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        total_loss += loss\n",
    "        if i>10: break # For now we will only use a subset of the data\n",
    "    return total_loss / len(trainloader)\n",
    "\n",
    "g = torch.autograd.grad([loss_fn(flat_p)],flat_p)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Linear Regression\n",
    "Implement linear regression with a heteroscedastic noise model where $\\Phi$ is the design matrix, $\\beta$ are the parameters and $\\sigma_i$ is the measurement noise. The model is:\n",
    "\n",
    "$$y = \\Phi \\beta + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, D)$$\n",
    "\n",
    "where $D$ is a diagonal matrix with $\\sigma_i^2$ on the diagonal. Add a Gaussian prior (regularization) if necessary.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: $\\hat{\\beta}_{MLE} = (\\Phi^T D^{-1} \\Phi)^{-1} \\Phi^T D^{-1} y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement pagerank to find the most influential pages of Wikipedia.\n",
    " From the transition matrix on the [Linked- WikiText-2 dataset](https://rloganiv.github.io/linked-wikitext-2/#/), compute the largest eigenvector using `cola.eigmax`. From this eigenvector, rank the values to determine which web pages are most influential.\n",
    "\n",
    "The PageRank algorithm computes the stationary distribution of a Markov chain. Given a transition matrix $P$, the PageRank vector $r$ is the eigenvector corresponding to the largest eigenvalue (which should be 1 for a stochastic matrix).\n",
    "\n",
    "The transition matrix $P$ is defined as:\n",
    "\n",
    "$$P = (1-\\alpha)W + \\alpha \\mathbf{1}\\mathbf{1}^T$$\n",
    "\n",
    "where $W$ is the adjacency matrix normalized by the degree, $\\alpha$ is the damping factor (usually set to 0.15), and $\\mathbf{1}$ is a vector of ones.\n",
    "\n",
    "The adjacency matrix $A_{ij}$ is 1 if there is a link from page $i$ to page $j$ (not the other way around). The degree-normalized adjacency matrix $W$ is obtained by dividing each row of $A$ by its sum.\n",
    "\n",
    "The PageRank vector $r$ can be found by solving the eigenproblem:\n",
    "\n",
    "$$P^T r = r$$\n",
    "\n",
    "The entries of $r$ give the PageRank scores of the pages. The pages can then be ranked by these scores to find the most influential ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some starter code to create an adjacency matrix. The pages are in the form of Wikipedia QIDs. After finding the most popular QIDs, if they are not in the `page_to_title dict`, you can look them up using the wikipedia API with the `get_titles_from_wikidata` function.\n",
    "\n",
    "Suggestion: use the `cola.ops.Sparse` matrix for the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /data/users/marc_f/miniconda3/envs/gr/lib/python3.10/site-packages (2.28.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /data/users/marc_f/miniconda3/envs/gr/lib/python3.10/site-packages (from requests) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /data/users/marc_f/miniconda3/envs/gr/lib/python3.10/site-packages (from requests) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/users/marc_f/miniconda3/envs/gr/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/users/marc_f/miniconda3/envs/gr/lib/python3.10/site-packages (from requests) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement io (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for io\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement zipfile (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for zipfile\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip --quiet install git+https://github.com/wilson-labs/cola.git\n",
    "%pip install requests\n",
    "%pip install io\n",
    "%pip install zipfile\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import requests, zipfile, io\n",
    "r = requests.get(\"https://rloganiv.github.io/linked-wikitext-2/static/media/linked-wikitext-2.142e2e52.zip\")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "\n",
    "# Extract the JSONL file\n",
    "\n",
    "\n",
    "# Initialize a dictionary to hold the adjacency list\n",
    "adjacency_list = defaultdict(set)\n",
    "\n",
    "# Initialize a dictionary to map page ids to indices\n",
    "page_to_index = {}\n",
    "index_to_page = {}\n",
    "page_to_title = {}\n",
    "next_index = 0\n",
    "\n",
    "\n",
    "files = ['valid.jsonl', 'train.jsonl', 'test.jsonl']\n",
    "for file in files:\n",
    "  with z.open(file) as f:\n",
    "      data = f.read().decode()\n",
    "      for line in data.splitlines():\n",
    "          data = json.loads(line)\n",
    "          current_page_id = data['annotations'][0]['id']\n",
    "          \n",
    "          page_to_title[current_page_id] = data['title']\n",
    "          # If the current page id is not in the dictionary, add it\n",
    "          if current_page_id not in page_to_index:\n",
    "              page_to_index[current_page_id] = next_index\n",
    "              index_to_page[next_index] = current_page_id\n",
    "              next_index += 1\n",
    "\n",
    "          current_page_index = page_to_index[current_page_id]\n",
    "          for annotation in data['annotations']:\n",
    "              # If the annotation is a link to another page, add it to the adjacency list\n",
    "              if annotation['source'] == 'WIKI' and annotation['id'] != current_page_id:\n",
    "                  linked_page_id = annotation['id']\n",
    "\n",
    "                  # If the linked page id is not in the dictionary, add it\n",
    "                  if linked_page_id not in page_to_index:\n",
    "                      page_to_index[linked_page_id] = next_index\n",
    "                      index_to_page[next_index] = linked_page_id\n",
    "                      next_index += 1\n",
    "\n",
    "                  linked_page_index = page_to_index[linked_page_id]\n",
    "                  adjacency_list[current_page_index].add(linked_page_index)\n",
    "\n",
    "def get_titles_from_wikidata(qids):\n",
    "    qids_string = '|'.join(qids)\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'ids': qids_string,\n",
    "        'format': 'json',\n",
    "        'props': 'labels',\n",
    "        'languages': 'en'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    titles = {}\n",
    "    for qid, entity in data['entities'].items():\n",
    "        if 'en' in entity['labels']:\n",
    "            titles[qid] = entity['labels']['en']['value']\n",
    "    return titles\n",
    "\n",
    "num_pages = len(page_to_index)\n",
    "adjacency_matrix = np.zeros((num_pages, num_pages), dtype=int)\n",
    "for page_index, linked_page_indices in adjacency_list.items():\n",
    "    for linked_page_index in linked_page_indices:\n",
    "        adjacency_matrix[page_index, linked_page_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45051, 45051)\n"
     ]
    }
   ],
   "source": [
    "print(adjacency_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Make a pull request to CoLA.\n",
    " e.g., improvement to the documentation, new commonly used linear operator (e.g., Fisher information matrix, banded matrix, FFT matrix), bug fix. If your code for one of the above exercises is particularly clean, consider adding markdown text explaining the steps and let's add it to the CoLA documentation under examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
