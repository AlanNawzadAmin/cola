{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# CoLA Library Exercise\n",
    "\n",
    "This exercise will help you get familiar with the CoLA. \n",
    "\n",
    "## Installation\n",
    "You need a Python 3.10+ environment with either JAX or PyTorch installed (or both). You can install CoLA using pip as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/wilson-labs/cola.git\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alternatively, you can open the documentation in Colab and start working from there: [Quick Start](https://colab.research.google.com/github/wilson-labs/cola/blob/master/docs/notebooks/colabs/Quick_Start.ipynb)\n",
    "\n",
    "We strongly recommend that you read through the [documentation](https://cola.readthedocs.io/en/latest/index.html) to understand the library better.\n",
    "\n",
    "## Basic Exercises\n",
    "We'll start with some basic exercises to get you warmed-up for later. For each of the following, you can use either JAX or PyTorch. We recommend that you try both and see if you spot any difference on the behavior of CoLA.\n",
    "\n",
    "As explained in [Linear Operators: What and Why?](https://cola.readthedocs.io/en/latest/notebooks/LinOpIntro.html) `LinearOperators` are an efficient and scalable manner to represent matrices. To illustrate how to use the `LinearOperators` available in `CoLA`, I'm going to create a `Dense` `LinearOperator` using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense\n",
      "<class 'cola.ops.operators.Dense'>\n",
      "torch.float32\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import cola\n",
    "import torch\n",
    "\n",
    "dtype = torch.float32\n",
    "A = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], dtype=dtype)\n",
    "A_op = cola.ops.Dense(A)\n",
    "print(A_op)\n",
    "print(type(A_op))\n",
    "print(A_op.dtype)\n",
    "print(A_op.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the prints, `A_op` is no longer a `torch.tensor`!\n",
    "\n",
    "The `LinearOperator` class is quite simple as it only requires that we define three minimum requirements: (1) a `dtype`, (2) a `shape` and (3) a `matmul` function. In the dense case above, all these requirements are taken from the matrix $A$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6., 15., 24.])\n",
      "tensor([ 6., 15., 24.])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(size=(A.shape[0],), dtype=dtype)\n",
    "print(A_op @ ones)\n",
    "print(A @ ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases we get the same results and, notably, we can use the same syntax that we are accustomed to!\n",
    "You can find all the available operators [here](https://cola.readthedocs.io/en/latest/package/cola.ops.html).\n",
    "\n",
    "Now, create a diagonal `LinearOperator` using the `cola.ops.Diagonal` class, where the diagonal should be equal to $d=(-1, 2, 3)^T$ and a tridiagonal `LinearOperator` with a diagonal full of ones and the upper and lower bands being $\\alpha=(-1/2, 1/4)^T$. _Hint:_ Should $\\beta$ had the same shape as $\\alpha$?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal = torch.tensor(_, dtype=dtype)\n",
    "D_op = cola.ops.Diagonal(_)\n",
    "alpha = torch.tensor(_, dtype=dtype)\n",
    "beta = _\n",
    "T_op = cola.ops.Tridiagonal(alpha, beta, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dense version of each of these `LinearOperators` using the `to_dense()` method and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = _ \n",
    "T = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, perform some basic operations like addition, subtraction, and multiplication on the previous operators. Verify that the computations are correct using the dense API. I suggested one set of binary operations below, but please try more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_)\n",
    "print(A + D - T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([3, 3])\n",
      "tensor([[ 1.0000,  0.5000,  0.0000],\n",
      "        [-0.5000,  1.0000, -0.2500],\n",
      "        [ 0.0000,  0.2500,  1.0000]])\n",
      "tensor([ 6., 15., 24.])\n",
      "tensor([ 6., 15., 24.])\n",
      "Dense+diag(tensor([-1.,  2.,  3.]))+-1*Tridiagonal\n",
      "tensor([[-1.0000,  1.5000,  3.0000],\n",
      "        [ 4.5000,  6.0000,  6.2500],\n",
      "        [ 7.0000,  7.7500, 11.0000]])\n",
      "tensor([[-1.0000,  1.5000,  3.0000],\n",
      "        [ 4.5000,  6.0000,  6.2500],\n",
      "        [ 7.0000,  7.7500, 11.0000]])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "A = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]], dtype=dtype)\n",
    "ones = torch.ones(size=(A.shape[0],), dtype=dtype)\n",
    "A_op = cola.ops.Dense(A)\n",
    "D_op = cola.ops.Diagonal(torch.tensor([-1., 2., 3.], dtype=dtype))\n",
    "alpha = torch.tensor([[-0.5, 0.25]], dtype=dtype).T\n",
    "beta = torch.tensor([[1., 1., 1.]], dtype=dtype).T\n",
    "T_op = cola.ops.Tridiagonal(alpha, beta, -alpha)\n",
    "\n",
    "\n",
    "print(A_op.dtype)\n",
    "print(A_op.shape)\n",
    "\n",
    "D = D_op.to_dense()\n",
    "T = T_op.to_dense()\n",
    "print(T_op.to_dense())\n",
    "\n",
    "print(A_op @ ones)\n",
    "print(A @ ones)\n",
    "\n",
    "print(A_op + D_op - T_op)\n",
    "print((A_op + D_op - T_op).to_dense())\n",
    "print(A + D - T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably the most popular linear algebra operation is solving a linear system, the so called \"solves\". You are now going to solve a linear system generated from a random matrix using CoLA. What function should you use from the [high level interface](https://cola.readthedocs.io/en/latest/package/cola.linalg.html)? Here is some code to create a random problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "from jax.random import PRNGKey, normal, split\n",
    "\n",
    "N = _\n",
    "key = PRNGKey(seed=21)\n",
    "A = normal(key, shape=(N, N))\n",
    "key = split(key, num=1)\n",
    "rhs = normal(key, shape=(N,))\n",
    "rhs /= jnp.linalg.norm(rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a dense `LinearOperator` and find the solution. Compare it to the solution found using `JAX`'s own solver implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln = _\n",
    "soln_jax = jnp.linalg.solve(A, rhs)\n",
    "abs_diff = jnp.linalg.norm(_)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What floating point precision was used in the previous case? Is this relevant? Why or why not?\n",
    "\n",
    "_[Answer Here]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.31e-06\n"
     ]
    }
   ],
   "source": [
    "from jax import numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "from jax.random import normal\n",
    "from jax.random import split\n",
    "\n",
    "N = 100\n",
    "key = PRNGKey(seed=21)\n",
    "A = normal(key, shape=(N, N))\n",
    "A_op = cola.ops.Dense(A)\n",
    "key = split(key, num=1)\n",
    "rhs = normal(key, shape=(N,))\n",
    "rhs /= jnp.linalg.norm(rhs)\n",
    "\n",
    "soln = cola.inverse(A_op) @ rhs\n",
    "\n",
    "soln_jax = jnp.linalg.solve(A, rhs)\n",
    "abs_diff = jnp.linalg.norm(soln - soln_jax)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, CoLA dispatched a general linear solver as it did not have any extra information about the `LinearOperator` that it could exploit. Lets see how to add some information about the operator such as being PSD, self-adjoint or symmetric if real.\n",
    "In CoLA we do this using annotation operators like `cola.ops.PSD`, `cola.ops.SelfAdjoint` and `cola.ops.Symmetric`. Let's work with a PSD matrix now.\n",
    "Construct a `LinearOperator` $S = A A^T + \\mu I$. _Hint_: make $A$ dense, use `cola.ops.I_like` (see [docs](https://cola.readthedocs.io/en/latest/package/cola.ops.html#cola.ops.I_like)) and forget that you are using CoLA. Don't forget to annotate your operator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "N = 1_000\n",
    "key = PRNGKey(seed=21)\n",
    "dtype = jnp.float64\n",
    "A = cola.ops.Dense(normal(key, shape=(N, N)))\n",
    "mu = 1.e-1  # a large enough value ensures PSD\n",
    "S = _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a PSD `LinearOperator` opens up the possiblity in CoLA to dispatch our favorite algorithm: CG. As we all know, CG has a couple of hyperparameters like the tolerance (set it to $10^{-10}$) and the max number of iterations (set it to 10K). _Hint_: Pass `method`, `tol` and `max_iters` to the function that you used in the previous exercise. To see what is going on under the hood, take a look at the [source code](https://github.com/wilson-labs/cola/blob/main/cola/linalg/inverse.py#L67)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln = _ @ rhs\n",
    "soln_jax = jnp.linalg.solve(S, rhs)\n",
    "abs_diff = jnp.linalg.norm(soln - soln_jax)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the difference much smaller that in the previous exercise?\n",
    "\n",
    "_[Answer Here]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.62e-13\n"
     ]
    }
   ],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "N = 100\n",
    "key = PRNGKey(seed=21)\n",
    "dtype = jnp.float64\n",
    "A = normal(key, shape=(N, N))\n",
    "mu = 1.e-1  # a large enough value ensures PSD\n",
    "S = A @ A.T + mu * jnp.eye(A.shape[0])\n",
    "# S_op = cola.ops.Dense(S)  # compare to option below\n",
    "S_op = cola.ops.PSD(cola.ops.Dense(S))\n",
    "key = split(key, num=1)\n",
    "rhs = normal(key, shape=(N,))\n",
    "rhs /= jnp.linalg.norm(rhs)\n",
    "\n",
    "soln = cola.inverse(S_op, method=\"cg\", tol=1e-10, max_iters=10_000) @ rhs\n",
    "\n",
    "soln_jax = jnp.linalg.solve(S, rhs)\n",
    "abs_diff = jnp.linalg.norm(soln - soln_jax)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude the basic set of exercises lets focus on another fundamental linear algebra operation: eigendecomposition. For this case, use the $T$ matrix used here: \n",
    "[Linear Operators: What and Why?](https://github.com/wilson-labs/cola/blob/main/docs/notebooks/LinOpIntro.ipynb). Get the eigendecomposition of $T$ using double precision and compare it with PyTorch or JAX's implementation. _Hint_: Check CoLA's [API](https://cola.readthedocs.io/en/latest/package/cola.linalg.html) and maybe use an annotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "dtype = _\n",
    "abs_diff = _\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are different algorithms being used? Is there a runtime benefit from dispatching a different algorithm?\n",
    "\n",
    "_[Answer Here]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00e+00\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "dtype = torch.float64\n",
    "alpha = torch.ones(size=(N - 1, 1), dtype=dtype)\n",
    "beta = -2 * torch.ones(size=(N, 1), dtype=dtype)\n",
    "T_op = cola.ops.Tridiagonal(alpha, beta, alpha)\n",
    "eigvals, eigvecs = cola.eig(T_op)\n",
    "eigvals_torch, eigvecs_torch = torch.linalg.eig(T_op.to_dense())\n",
    "abs_diff = torch.linalg.norm(eigvals - eigvals_torch)\n",
    "print(f\"{abs_diff:1.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Large Scale Machine Learning with CoLA\n",
    "\n",
    "Using JAX or PyTorch, pick any 3 out of the 5:\n",
    "\n",
    "### 1. GP\n",
    "\n",
    "GP Implement Gaussian Process (GP) inference with Radial Basis Function (RBF) kernel using `inverse()` from scratch on a dataset with at least 10k observations. You are not allowed to use GPyTorch. The formula for the GP posterior is:\n",
    "\n",
    "$$f_* | X, y, X_* \\sim \\mathcal{N}(\\mu_*, \\Sigma_*)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\mu_* = K(X_*, X)[K(X, X) + \\sigma^2_n I]^{-1}y$$\n",
    "\n",
    "$$\\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \\sigma^2_n I]^{-1}K(X, X_*)$$\n",
    "\n",
    "Here, $K$ is the RBF kernel, $X$ are the training inputs, $y$ are the training targets, $X_*$ are the test inputs, and $\\sigma^2_n$ is the noise variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O bike.mat \"https://www.andpotap.com/static/bike.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "import os\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from scipy.io import loadmat\n",
    "import cola\n",
    "\n",
    "\n",
    "def load_uci_data(data_dir, dataset, train_p=0.75, test_p=0.15):\n",
    "    file_path = os.path.join(data_dir, dataset + '.mat')\n",
    "    data = np.array(loadmat(file_path)['data'])\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "\n",
    "    X = X - X.min(0)[None]\n",
    "    X = 2.0 * (X / X.max(0)[None]) - 1.0\n",
    "    y -= y.mean()\n",
    "    y /= y.std()\n",
    "\n",
    "    train_n = int(floor(train_p * X.shape[0]))\n",
    "    valid_n = int(floor((1. - train_p - test_p) * X.shape[0]))\n",
    "\n",
    "    split = split_dataset(X, y, train_n, valid_n)\n",
    "    train_x, train_y, valid_x, valid_y, test_x, test_y = split\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, valid_x, valid_y\n",
    "\n",
    "\n",
    "def split_dataset(x, y, train_n, valid_n):\n",
    "    train_x = x[:train_n, :]\n",
    "    train_y = y[:train_n]\n",
    "\n",
    "    valid_x = x[train_n:train_n + valid_n, :]\n",
    "    valid_y = y[train_n:train_n + valid_n]\n",
    "\n",
    "    test_x = x[train_n + valid_n:, :]\n",
    "    test_y = y[train_n + valid_n:]\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
    "\n",
    "\n",
    "train_x, train_y, *_, test_x, test_y = load_uci_data(data_dir=\"./\", dataset=\"bike\")\n",
    "\n",
    "dtype = jnp.float32\n",
    "train_x, train_y = jnp.array(train_x[:100], dtype=dtype), jnp.array(train_y[:100], dtype=dtype)\n",
    "test_x, test_y = jnp.array(test_x[:10], dtype=dtype), jnp.array(test_y[:10], dtype=dtype)\n",
    "# train_x, train_y = jnp.array(train_x, dtype=dtype), jnp.array(train_y, dtype=dtype)\n",
    "# test_x, test_y = jnp.array(test_x, dtype=dtype), jnp.array(test_y, dtype=dtype)\n",
    "\n",
    "\n",
    "def compute_rbf_cov(xi, xj):\n",
    "    xi, xj = jnp.expand_dims(xi, -2), jnp.expand_dims(xj, -3)\n",
    "    res = jnp.exp(-jnp.array(0.5, dtype=xi.dtype) * jnp.sum((xi - xj) ** 2., axis=-1))\n",
    "    return res\n",
    "\n",
    "\n",
    "ls = jnp.array(100., dtype=dtype)\n",
    "noise = jnp.array(1., dtype=dtype)\n",
    "oscale = jnp.array(1., dtype=dtype)\n",
    "K_train_train = cola.ops.Dense(oscale * compute_rbf_cov(train_x / ls, train_x / ls))\n",
    "K_test_train = cola.ops.Dense(oscale * compute_rbf_cov(test_x / ls, train_x / ls))\n",
    "K_test_test = cola.ops.Dense(oscale * compute_rbf_cov(test_x / ls, test_x / ls))\n",
    "K = cola.ops.PSD(K_train_train + noise * cola.ops.I_like(K_train_train))\n",
    "mu = K_test_train @ cola.inverse(K) @ train_y\n",
    "Sigma = K_test_test.to_dense() - K_test_train @ (cola.inverse(K)) @ K_test_train.T\n",
    "# Sigma = K_test_test - K_test_train @ (cola.inverse(K)) @ K_test_train.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Hessian Spectrum\n",
    "Compute the eigenspectrum of the Hessian of a pretrained neural network. You can download weights of image classifiers pretrained on CIFAR10. Use `cola.eig` or `cola.algorithms.lanczsos` and the spectral KDE smoothing method from [this paper](https://arxiv.org/pdf/1901.10159.pdf) to get a smoothed spectrum estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170498071/170498071 [00:07<00:00, 24219391.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubu/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n",
      "/home/ubu/venv/cola/lib/python3.11/site-packages/torch/nn/utils/stateless.py:216: UserWarning: This API is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use torch.func.functional_call instead which is a drop-in replacement for this API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils import stateless\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                                        transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, shuffle=False, num_workers=2)\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)\n",
    "model = model.to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def flatten_params(params):\n",
    "    shapes = [p.shape for p in params]\n",
    "    flat_params = torch.cat([p.flatten() for p in params])\n",
    "    return flat_params, shapes\n",
    "\n",
    "\n",
    "def unflatten_params(flat_params, shapes):\n",
    "    params = []\n",
    "    i = 0\n",
    "    for shape in shapes:\n",
    "        size = torch.prod(torch.tensor(shape)).item()\n",
    "        param = flat_params[i:i + size]\n",
    "        param = param.view(shape)\n",
    "        params.append(param)\n",
    "        i += size\n",
    "    return params\n",
    "\n",
    "\n",
    "flat_p, shape = flatten_params(list(model.parameters()))\n",
    "flat_p = flat_p.detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "def stateless_model(fparams, x):\n",
    "    params = unflatten_params(fparams, shape)\n",
    "    names = list(n for n, _ in model.named_parameters())\n",
    "    nps = {n: p for n, p in zip(names, params)}\n",
    "    return stateless.functional_call(model, nps, x)\n",
    "\n",
    "\n",
    "def loss_fn(params):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    total_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        outputs = stateless_model(params, images.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        total_loss += loss\n",
    "        if i > 10:\n",
    "            break  # For now we will only use a subset of the data\n",
    "    return total_loss / len(trainloader)\n",
    "\n",
    "\n",
    "g = torch.autograd.grad([loss_fn(flat_p)], flat_p)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = torch.autograd.grad([loss_fn(flat_p)], flat_p, create_graph=True)\n",
    "v = torch.ones(size=(grads[0].shape[0],))\n",
    "\n",
    "\n",
    "def matmat(vec):\n",
    "    if len(vec.shape) == 1:\n",
    "        vec = vec.reshape(-1, 1)\n",
    "    hessian_vector_product = []\n",
    "    for idx in range(vec.shape[1]):\n",
    "        inner_grad = torch.autograd.grad(grads, flat_p, vec[:, idx], retain_graph=True)[0]\n",
    "        hessian_vector_product.append(inner_grad)\n",
    "\n",
    "    hessian_mvm = torch.stack(hessian_vector_product).T\n",
    "    return hessian_mvm\n",
    "\n",
    "\n",
    "shape = (flat_p.shape[0], flat_p.shape[0])\n",
    "Hess = cola.ops.LinearOperator(shape=shape, dtype=v.dtype, matmat=matmat)\n",
    "# Hess = cola.ops.Symmetric(Hess)\n",
    "print(Hess @ v)\n",
    "out = cola.algorithms.lanczos(Hess, flat_p, max_iters=10, tol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Linear Regression\n",
    "Implement linear regression with a heteroscedastic noise model where $\\Phi$ is the design matrix, $\\beta$ are the parameters and $\\sigma_i$ is the measurement noise. The model is:\n",
    "\n",
    "$$y = \\Phi \\beta + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, D)$$\n",
    "\n",
    "where $D$ is a diagonal matrix with $\\sigma_i^2$ on the diagonal. Add a Gaussian prior (regularization) if necessary.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: $\\hat{\\beta}_{MLE} = (\\Phi^T D^{-1} \\Phi)^{-1} \\Phi^T D^{-1} y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0178,  0.0054, -0.0097,  0.0379,  0.0159,  0.0046,  0.6153, -0.4644,\n",
      "         0.0087, -0.1270, -0.2287,  0.9366, -0.5653, -0.1517, -0.2609, -0.3012,\n",
      "         1.4519])\n",
      "Test RMSE: 2.409e+01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_x, train_y, *_, test_x, test_y = load_uci_data(data_dir=\"./\", dataset=\"bike\")\n",
    "\n",
    "dtype = torch.float32\n",
    "# train_x = torch.tensor(train_x[:100], dtype=dtype)\n",
    "# train_y = torch.tensor(train_y[:100], dtype=dtype)\n",
    "# test_x, test_y = torch.tensor(test_x[:10], dtype=dtype), torch.tensor(test_y[:10], dtype=dtype)\n",
    "train_x, train_y = torch.tensor(train_x, dtype=dtype), torch.tensor(train_y, dtype=dtype)\n",
    "test_x, test_y = torch.tensor(test_x, dtype=dtype), torch.tensor(test_y, dtype=dtype)\n",
    "\n",
    "sigma = 0.1\n",
    "sigma_x = sigma * (1. + torch.norm(train_x, dim=1))\n",
    "# sigma_x = sigma * torch.norm(train_x, dim=1)\n",
    "# sigma_x = torch.ones(size=(train_x.shape[0],))\n",
    "D = cola.ops.Diagonal(sigma_x)\n",
    "Phi = cola.ops.Dense(train_x)\n",
    "mu = 0.05\n",
    "Phi_inv = Phi.T @ cola.inverse(D) @ Phi\n",
    "Phi_inv += mu * cola.ops.I_like(Phi_inv)\n",
    "Phi_inv = cola.inverse(cola.ops.PSD(Phi_inv))\n",
    "beta_mle = Phi_inv @ Phi.T @ cola.inverse(D) @ train_y\n",
    "print(beta_mle)\n",
    "\n",
    "test_rmse = torch.norm(test_y - test_x @ beta_mle)\n",
    "print(f\"Test RMSE: {test_rmse:1.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement pagerank to find the most influential pages of Wikipedia.\n",
    " From the transition matrix on the [Linked- WikiText-2 dataset](https://rloganiv.github.io/linked-wikitext-2/#/), compute the largest eigenvector using `cola.eigmax`. From this eigenvector, rank the values to determine which web pages are most influential.\n",
    "\n",
    "The PageRank algorithm computes the stationary distribution of a Markov chain. Given a transition matrix $P$, the PageRank vector $r$ is the eigenvector corresponding to the largest eigenvalue (which should be 1 for a stochastic matrix).\n",
    "\n",
    "The transition matrix $P$ is defined as:\n",
    "\n",
    "$$P = (1-\\alpha)W + \\alpha \\mathbf{1}\\mathbf{1}^T$$\n",
    "\n",
    "where $W$ is the adjacency matrix normalized by the degree, $\\alpha$ is the damping factor (usually set to 0.15), and $\\mathbf{1}$ is a vector of ones.\n",
    "\n",
    "The adjacency matrix $A_{ij}$ is 1 if there is a link from page $i$ to page $j$ (not the other way around). The degree-normalized adjacency matrix $W$ is obtained by dividing each row of $A$ by its sum.\n",
    "\n",
    "The PageRank vector $r$ can be found by solving the eigenproblem:\n",
    "\n",
    "$$P^T r = r$$\n",
    "\n",
    "The entries of $r$ give the PageRank scores of the pages. The pages can then be ranked by these scores to find the most influential ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some starter code to create an adjacency matrix. The pages are in the form of Wikipedia QIDs. After finding the most popular QIDs, if they are not in the `page_to_title dict`, you can look them up using the wikipedia API with the `get_titles_from_wikidata` function.\n",
    "\n",
    "Suggestion: use the `cola.ops.Sparse` matrix for the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip --quiet install git+https://github.com/wilson-labs/cola.git\n",
    "%pip install requests\n",
    "%pip install io\n",
    "%pip install zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cola\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Request the zipped data\n",
    "link = \"https://rloganiv.github.io/linked-wikitext-2/static/media/linked-wikitext-2.142e2e52.zip\"\n",
    "r = requests.get(link)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "\n",
    "# Initialize a dictionary to hold the adjacency list\n",
    "adjacency_list = defaultdict(set)\n",
    "\n",
    "# Initialize a dictionary to map page ids to indices\n",
    "page_to_index, index_to_page, page_to_title, next_index = {}, {}, {}, 0\n",
    "\n",
    "# Extract the JSONL file\n",
    "files = ['valid.jsonl', 'train.jsonl', 'test.jsonl']\n",
    "for file in files:\n",
    "    with z.open(file) as f:\n",
    "        data = f.read().decode()\n",
    "        for line in data.splitlines():\n",
    "            data = json.loads(line)\n",
    "            current_page_id = data['annotations'][0]['id']\n",
    "\n",
    "            page_to_title[current_page_id] = data['title']\n",
    "            # If the current page id is not in the dictionary, add it\n",
    "            if current_page_id not in page_to_index:\n",
    "                page_to_index[current_page_id] = next_index\n",
    "                index_to_page[next_index] = current_page_id\n",
    "                next_index += 1\n",
    "\n",
    "            current_page_index = page_to_index[current_page_id]\n",
    "            for annotation in data['annotations']:\n",
    "                # If the annotation is a link to another page, add it to the adjacency list\n",
    "                if (annotation['source'] == 'WIKI') and (annotation['id'] != current_page_id):\n",
    "                    linked_page_id = annotation['id']\n",
    "\n",
    "                    # If the linked page id is not in the dictionary, add it\n",
    "                    if linked_page_id not in page_to_index:\n",
    "                        page_to_index[linked_page_id] = next_index\n",
    "                        index_to_page[next_index] = linked_page_id\n",
    "                        next_index += 1\n",
    "\n",
    "                    linked_page_index = page_to_index[linked_page_id]\n",
    "                    adjacency_list[current_page_index].add(linked_page_index)\n",
    "\n",
    "num_pages = len(page_to_index)\n",
    "adjacency_matrix = np.zeros((num_pages, num_pages), dtype=int)\n",
    "for page_index, linked_page_indices in adjacency_list.items():\n",
    "    for linked_page_index in linked_page_indices:\n",
    "        adjacency_matrix[page_index, linked_page_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([47,  0, 96, 73, 75, 72, 71, 70, 69, 68])\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "adjacency_matrix = adjacency_matrix[:100, :100]\n",
    "adjacency_matrix = torch.tensor(adjacency_matrix, dtype=dtype)\n",
    "ones = cola.ops.Dense(torch.ones(size=(adjacency_matrix.shape[0], 1)))\n",
    "norm = adjacency_matrix.sum(0).unsqueeze(1)\n",
    "norm = torch.where(norm == 0., torch.tensor(1.), norm)\n",
    "W = cola.ops.Dense(adjacency_matrix / norm)\n",
    "alpha = 0.15\n",
    "P = (1. - alpha) * W + alpha * ones @ ones.T\n",
    "eigvec_max, eig_max = cola.eigmax(P)\n",
    "_, indices = torch.sort(eigvec_max, descending=True)\n",
    "print(indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_from_wikidata(qids):\n",
    "    qids_string = '|'.join(qids)\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'wbgetentities',\n",
    "        'ids': qids_string,\n",
    "        'format': 'json',\n",
    "        'props': 'labels',\n",
    "        'languages': 'en'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    titles = {}\n",
    "    for qid, entity in data['entities'].items():\n",
    "        if 'en' in entity['labels']:\n",
    "            titles[qid] = entity['labels']['en']['value']\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Make a pull request to CoLA.\n",
    " e.g., improvement to the documentation, new commonly used linear operator (e.g., Fisher information matrix, banded matrix, FFT matrix), bug fix. If your code for one of the above exercises is particularly clean, consider adding markdown text explaining the steps and let's add it to the CoLA documentation under examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
